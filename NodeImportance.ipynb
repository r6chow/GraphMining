{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3  -  Roger Chow (r6chow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import warnings\n",
    "import math\n",
    "import operator\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep: Load political blog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read edge list into Directed graph\n",
    "pol_DiG=nx.read_edgelist(\"polblogs.edgelist.simple_format_unweighted\", \n",
    "                   nodetype=str,\n",
    "                   data=(('weight',float),),\n",
    "                   create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Implement Closeness Centrality from scratch (7)\n",
    "\n",
    "The implementation of Closeness Centrality requires the identification of shortest path between nodes.  We will use Dijkstra implementation from Homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Closeness_Centrality(G, N=None):\n",
    "    \n",
    "    closeness = {}\n",
    "    \n",
    "    for n in G.nodes():\n",
    "    \n",
    "        #get the shortest path from node N\n",
    "        dist_dict = Dijkstra(G, n)\n",
    "\n",
    "        sum_distance = 0\n",
    "\n",
    "        #sum the n-1 distances reachable from N\n",
    "        for node in dist_dict:\n",
    "            if node != N:\n",
    "                sum_distance += len(dist_dict[node])-1\n",
    "\n",
    "        #closeness centrality (week 3 lecture) \n",
    "        # CLV(v) = [|V| - 1] / [SUM(distance(v, vi))]\n",
    "\n",
    "        if sum_distance > 0 :\n",
    "            closeness[n] = ((len(dist_dict) -1) / sum_distance )\n",
    "        else:\n",
    "            closeness[n] = 0\n",
    "    \n",
    "    if N is not None:\n",
    "        return closeness[N]\n",
    "    else:\n",
    "        return closeness\n",
    "    \n",
    "    \n",
    "\n",
    "# Return a dictionary of shortest path from Node N (Homework 2)\n",
    "def Dijkstra(G, N):\n",
    "    \n",
    "    # for directed Graph, we need to reverse the view\n",
    "    if G.is_directed():\n",
    "        G = G.reverse()\n",
    "        \n",
    "    unvisited_nodes = list(G.nodes())\n",
    "    visited_nodes = []\n",
    "    distances = {}\n",
    "    prev = {}\n",
    "\n",
    "    #initialize distances of all nodes from N as infinity\n",
    "    for n in unvisited_nodes:\n",
    "        distances[n] = math.inf\n",
    "        prev[n] = ''\n",
    "\n",
    "    #initialize distance of node N as 0\n",
    "    distances[N] = 0\n",
    "\n",
    "    while (len(unvisited_nodes) > 0):\n",
    "\n",
    "        #pick node X from unvisited array with shortest distance from N\n",
    "        min_dist = min([distances[x] for x in unvisited_nodes])\n",
    "        X = ''\n",
    "        for x in unvisited_nodes:\n",
    "            if (distances[x] <= min_dist):\n",
    "                X = x\n",
    "\n",
    "        #for each vertex Y that is a neighbour of X\n",
    "        neighbors_X = [n for n in nx.neighbors(G, X)]\n",
    "        \n",
    "        for Y in neighbors_X:\n",
    "\n",
    "            #compute the distance of Y from N using X as an intermediary node\n",
    "            weight = 1  #constant for unweighted\n",
    "            \n",
    "            new_dist_Y = distances[X] + weight\n",
    "\n",
    "            #if the new_distance(Y) < old_distance(Y) then replace old_distance(Y) with new_distance(Y) and prev_vertex of Y with X\n",
    "            if (new_dist_Y < distances[Y]):\n",
    "                distances[Y] = new_dist_Y\n",
    "                prev[Y] = X\n",
    "\n",
    "        visited_nodes.append(X)\n",
    "        unvisited_nodes.remove(X)\n",
    "   \n",
    "\n",
    "    #shortest paths reachable\n",
    "    shortest_path = {}\n",
    "    for node in prev.keys():\n",
    "        path = getPath(node, prev)\n",
    "        if (path[0] == N):\n",
    "            shortest_path[node] = getPath(node, prev)\n",
    "    \n",
    "    #sort and return the dictionary of shortest paths for node N\n",
    "    return dict((x, y) for x, y in sorted(shortest_path.items(), key = lambda kv:len(kv[1])))\n",
    "\n",
    "\n",
    "#helper function to get the path to node n based on prev \n",
    "def getPath(n, prev):\n",
    "    if (prev[n] == ''):\n",
    "        return ([n])\n",
    "    else:\n",
    "        return (getPath(prev[n], prev))+[n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare implementation with Networkx implementation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Closeness_Centrality(pol_DiG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lashawnbarber.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(result.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['lashawnbarber.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185185185185185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['instapundit.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_result = nx.closeness_centrality(pol_DiG, wf_improved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lashawnbarber.com'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nx_result.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx_result['lashawnbarber.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185185185185185"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx_result['instapundit.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Implement Page Rank (using Damping factor approach) from scratch (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRank(G, d=0.85, tol=1e-05, max_iter=100):\n",
    "    \n",
    "    #initialize all page range to 1 / |V|\n",
    "    G_size = len(G.nodes())\n",
    "    curr_pagerank = dict.fromkeys(G.nodes(), 1/G_size)\n",
    "    \n",
    "    # convert to stochastic graph where the sum of out-edges adds to 1 \n",
    "    # [shortcut for getting connecting nodes contribution]\n",
    "    A = nx.stochastic_graph(G)\n",
    "\n",
    "    # find all the dangling nodes (out degree is 0)\n",
    "    nodes_dangling = [node for node in A if A.out_degree(node) == 0.0] \n",
    "    \n",
    "    # iterate until max iterations reach or convergence reached\n",
    "    for i in range(max_iter):\n",
    "        prev_pagerank = curr_pagerank\n",
    "        curr_pagerank = dict.fromkeys(prev_pagerank, 0)\n",
    "        \n",
    "        #sum up the dangling nodes\n",
    "        sum_nodes_dangling = d * sum(prev_pagerank[node] for node in nodes_dangling) \n",
    "        \n",
    "        #compute new page rank for each node  i.e.  PR(A)/C(A) + ... \n",
    "        for node in curr_pagerank:\n",
    "            \n",
    "            for conn_node in A[node]:\n",
    "                curr_pagerank[conn_node] += d * prev_pagerank[node] * A[node][conn_node]['weight']\n",
    "                \n",
    "            #based on http://home.ie.cuhk.edu.hk/~wkshum/papers/pagerank.pdf, dangling node have probablity (1/n)\n",
    "            curr_pagerank[node] += (sum_nodes_dangling / G_size) +  ((1.0 - d) / G_size)\n",
    "        \n",
    "        #check for convergence if the total difference between previous and current page rank is less than tolerance\n",
    "        if sum([abs(curr_pagerank[node] - prev_pagerank[node]) for node in curr_pagerank]) < tol:\n",
    "            return  curr_pagerank\n",
    "        \n",
    "    return curr_pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare implementation with Networkx implementation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageRankResult = PageRank(pol_DiG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instapundit.com'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pageRankResult.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03157503121429926"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageRankResult['instapundit.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002737427388847285"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageRankResult['lashawnbarber.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_PageRankResult = nx.pagerank(pol_DiG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instapundit.com'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nx_PageRankResult.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03158762626582744"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx_PageRankResult['instapundit.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00027398331593247766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx_PageRankResult['lashawnbarber.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) How do the important nodes using the two methods compare? Are the node rankings the same or different? (1)\n",
    "\n",
    "<B>Answer:</B>  The important node found using Closeness Centralality is 'lashawnbarber.com'.  However, using Page Ranking with dampening factor of 0.85, the important node found was 'instapundit.com'.   If we compare how these two nodes ranked in other methods, 'instapundit.com' was still in the top 100 nodes importance (ranked 74 out of 1224) based on Closeness Centrality.  Conversly, the node 'lashawnbarber.com' was found to be at the bottom half of the page rankings (ranked 859 out of 1224).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_closeness = sorted(result.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lashawnbarber.com', 1.0),\n",
       " ('leonards-digest.blogspot.com', 1.0),\n",
       " ('tomburka.com', 1.0),\n",
       " ('bakshi.us/redskunk', 1.0),\n",
       " ('abbadabbaduo.blogspot.com', 1.0),\n",
       " ('stevemacek.blogspot.com', 1.0),\n",
       " ('lefttimes.com', 1.0),\n",
       " ('myleftbrain.com', 1.0),\n",
       " ('btcnews.com/btcnews', 1.0),\n",
       " ('paulchasman.com', 1.0),\n",
       " ('parabasis.typepad.com', 1.0),\n",
       " ('irregulartimes.com/blogger.html', 1.0),\n",
       " ('donkey2004.blogspot.com', 1.0),\n",
       " ('bushlies.net/pages/10/index.htm', 1.0),\n",
       " ('one38.org', 1.0),\n",
       " ('alvintostig.typepad.com', 1.0),\n",
       " ('radikal_papi.pitas.com', 1.0),\n",
       " ('radgeek.com', 1.0),\n",
       " ('erikack.blogspot.com', 1.0),\n",
       " ('theblueview.blogspot.com', 1.0),\n",
       " ('digital-democrat.blogspot.com', 1.0),\n",
       " ('quimundus.squarespace.com', 1.0),\n",
       " ('bullmooseblog.com', 1.0),\n",
       " ('thunewatch.squarespace.com', 1.0),\n",
       " ('simplyappalling.blogspot.com', 1.0),\n",
       " ('imprescindibile.ilcannocchiale.it', 1.0),\n",
       " ('ergio.blogspot.com', 1.0),\n",
       " ('enemykombatant.blogspot.com', 1.0),\n",
       " ('massachusetts-liberal.com', 1.0),\n",
       " ('grassrootsnation.com', 1.0),\n",
       " ('loveamericahatebush.com', 1.0),\n",
       " ('robschumacher.blogspot.com', 0.8333333333333334),\n",
       " ('wizbangblog.com', 0.7777777777777778),\n",
       " ('cayankee.blogs.com', 0.75),\n",
       " ('acepilots.com/mt', 0.75),\n",
       " ('battleground-wisconsin.blogspot.com', 0.75),\n",
       " ('nomoremister.blogspot.com', 0.7142857142857143),\n",
       " ('blogs.salon.com/0002874', 0.7),\n",
       " ('daddypundit.blogspot.com', 0.6666666666666666),\n",
       " ('rightonred.net', 0.6666666666666666)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_closeness[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pagerank = sorted(pageRankResult.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('instapundit.com', 0.03157503121429926),\n",
       " ('dailykos.com', 0.022109920485296256),\n",
       " ('right-thinking.com', 0.0162200808370096),\n",
       " ('rogerailes.blogspot.com', 0.014850212878956139),\n",
       " ('liberaloasis.com', 0.013919358539898674),\n",
       " ('michaelberube.com', 0.013197519511849318),\n",
       " ('prospect.org/weblog', 0.0124558513098053),\n",
       " ('denbeste.nu', 0.012119917396663644),\n",
       " ('kensain.com', 0.011963521496989751),\n",
       " ('mudvillegazette.com', 0.010940642821213415),\n",
       " ('dissectleft.blogspot.com', 0.010601908047248202),\n",
       " ('higherpieproductions.com', 0.010125738346325804),\n",
       " ('ladymac.com', 0.009925243300367417),\n",
       " ('blogsforbush.com', 0.009719367275369296),\n",
       " ('dolphinsdock.com/blog', 0.009589959267518015),\n",
       " ('pardonmyenglish.com', 0.009385947292756736),\n",
       " ('thepoorman.net', 0.009369571783782818),\n",
       " ('discourse.net', 0.008534771091938403),\n",
       " ('atrios.blogspot.com', 0.00812004834050041),\n",
       " ('writingcompany.blogs.com/this_isnt_writing_its_typ', 0.007735899484971417),\n",
       " ('counterspin.blogspot.com', 0.006848708925101928),\n",
       " ('therightcoast.blogspot.com', 0.006737234349743492),\n",
       " ('roseoftheknights.com/yank/pride.html', 0.006735540876512076),\n",
       " ('stcfoundation.blogspot.com', 0.006155036307099854),\n",
       " ('evangelicaloutpost.com', 0.006009204162168463),\n",
       " ('indcjournal.com', 0.00587048490998578),\n",
       " ('conservativelife.com/blog', 0.005868629633789011),\n",
       " ('norbizness.com', 0.005863781840071222),\n",
       " ('rogerlsimon.com', 0.005439094216529777),\n",
       " ('pinkofeministhellcat.typepad.com/pinko_feminist_hellcat',\n",
       "  0.005413506976911135),\n",
       " ('talkingpointsmemo.com', 0.005385151913701038),\n",
       " ('messopotamian.blogspot.com', 0.005114566319527003),\n",
       " ('realclearpolitics.com', 0.005014982860295039),\n",
       " ('markheimonen.blogspot.com', 0.004833457928910503),\n",
       " ('belmontclub.blogspot.com', 0.004770251033297213),\n",
       " ('qando.net', 0.0047536108611922175),\n",
       " ('michellemalkin.com', 0.004578754516745963),\n",
       " ('imao.us', 0.0044691370117876114),\n",
       " ('oipp.blogspot.com', 0.004313056561811282),\n",
       " ('righthandthief.blogspot.com', 0.004288258733512292)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_pagerank[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instapundit.com is at position 1 out of 1224\n",
      "lashawnbarber.com is at position 859 out of 1224\n"
     ]
    }
   ],
   "source": [
    "#find position of 'lashawnbarber.com' in pagerank\n",
    "size = len(sorted_pagerank)\n",
    "for i in range(len(sorted_pagerank)):\n",
    "    if (sorted_pagerank[i][0] == 'lashawnbarber.com'):\n",
    "        print('lashawnbarber.com is at position {0} out of {1}'.format(i+1, size))\n",
    "    if (sorted_pagerank[i][0] == 'instapundit.com'):\n",
    "        print('instapundit.com is at position {0} out of {1}'.format(i+1, size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lashawnbarber.com is at position 1 out of 1224\n",
      "instapundit.com is at position 74 out of 1224\n"
     ]
    }
   ],
   "source": [
    "#find position of 'lashawnbarber.com' in closeness\n",
    "size = len(sorted_closeness)\n",
    "for i in range(len(sorted_closeness)):\n",
    "    if (sorted_closeness[i][0] == 'lashawnbarber.com'):\n",
    "        print('lashawnbarber.com is at position {0} out of {1}'.format(i+1, size))\n",
    "    if (sorted_closeness[i][0] == 'instapundit.com'):\n",
    "        print('instapundit.com is at position {0} out of {1}'.format(i+1, size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
